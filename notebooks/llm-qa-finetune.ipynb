{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17cf7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9033e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8fb44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import transformers\n",
    "from transformers import (\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    AutoModel,\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForQuestionAnswering\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec5cd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a169885",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990acf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed9a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a67fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Union, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d728622",
   "metadata": {},
   "source": [
    "# Fine-Tuning for QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791733cf",
   "metadata": {},
   "source": [
    "## Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d7228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!pip install Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d013175",
   "metadata": {},
   "source": [
    "### Load Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cda6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "812f3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dbc629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ec42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c2a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_askathon_clean(path: str) -> pd.DataFrame:\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.drop(columns=[\"Email Address\"]).reset_index(drop=True)\n",
    "    data.rename(columns={\n",
    "        data.columns[0] : \"context\",\n",
    "        data.columns[1]: \"id\",\n",
    "        data.columns[2]: \"source\",\n",
    "        data.columns[3]: \"topics\",\n",
    "        data.columns[4]: \"q1\",\n",
    "        data.columns[5]: \"a1\",\n",
    "        data.columns[6]: \"q2\",\n",
    "        data.columns[7]: \"a2\",\n",
    "        data.columns[8]: \"q3\",\n",
    "        data.columns[9]: \"a3\",\n",
    "        data.columns[10]: \"q4\",\n",
    "        data.columns[11]: \"a4\",\n",
    "        data.columns[12]: \"q5\",\n",
    "        data.columns[13]: \"a5\"\n",
    "    }, inplace=True)\n",
    "    data.drop(columns=[\"source\", \"topics\"], inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8908e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_dataset(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    res = []\n",
    "    q_keys = [f\"q{i}\" for i in range(1, 6)]\n",
    "    a_keys = [f\"a{i}\" for i in range(1, 6)]\n",
    "    \n",
    "    def _index_fn(context: str, answer: str) -> int:\n",
    "        try:\n",
    "            return context.lower().index(answer.rstrip(\" ,.!?\").lower())\n",
    "        except ValueError:\n",
    "            return -1\n",
    "    \n",
    "    for _df in data.itertuples():\n",
    "        tmp = []\n",
    "        context = _df.context.strip()\n",
    "        for qk, ak in zip(q_keys, a_keys):\n",
    "            q, a = getattr(_df, qk), getattr(_df, ak)\n",
    "            \n",
    "            if not isinstance(a, str):\n",
    "                continue\n",
    "            idx = _index_fn(context, a)\n",
    "            if idx > -1:\n",
    "                tmp.append(dict(\n",
    "                    id=\"\".join(re.split(r\"[ :/]\", _df.id)),\n",
    "                    context=context,\n",
    "                    question=q,\n",
    "                    answer_text=a,\n",
    "                    answer_start=idx,\n",
    "                ))\n",
    "        res.extend(tmp)\n",
    "    return pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12248c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qa = create_qa_dataset(load_askathon_clean(\"data/qa/Askathon Cleaned responses - Form Responses 1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a19688f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (max(data_qa[\"context\"], key=lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb8adda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be easier for downstream preprocessing\n",
    "data_qa[\"answers\"] = data_qa[[\"answer_text\", \"answer_start\"]]\\\n",
    ".apply(lambda r: dict(text=[r[0]], answer_start=[r[1]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01bdd143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>992022143349</td>\n",
       "      <td>The environmental degradation depletes ecosyst...</td>\n",
       "      <td>What issue causes environmental degradation?</td>\n",
       "      <td>Pollution</td>\n",
       "      <td>51</td>\n",
       "      <td>{'text': ['Pollution'], 'answer_start': [51]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>992022143349</td>\n",
       "      <td>The environmental degradation depletes ecosyst...</td>\n",
       "      <td>What is Pollution is created from?</td>\n",
       "      <td>vehicle emissions, agricultural runoff, chemic...</td>\n",
       "      <td>162</td>\n",
       "      <td>{'text': ['vehicle emissions, agricultural run...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>992022143349</td>\n",
       "      <td>The environmental degradation depletes ecosyst...</td>\n",
       "      <td>What depletes ecosystems?</td>\n",
       "      <td>Environmental degradation</td>\n",
       "      <td>4</td>\n",
       "      <td>{'text': ['Environmental degradation '], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>992022143349</td>\n",
       "      <td>The environmental degradation depletes ecosyst...</td>\n",
       "      <td>What is a pollution source?</td>\n",
       "      <td>A variety of sources</td>\n",
       "      <td>130</td>\n",
       "      <td>{'text': ['A variety of sources'], 'answer_sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>912202292720</td>\n",
       "      <td>High-shear, low-CAPE (HSLC) severe convection ...</td>\n",
       "      <td>How would you characterize HSLC environments a...</td>\n",
       "      <td>Drier lower troposphere and a surface triple-p...</td>\n",
       "      <td>751</td>\n",
       "      <td>{'text': ['Drier lower troposphere and a surfa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                            context  \\\n",
       "0  992022143349  The environmental degradation depletes ecosyst...   \n",
       "1  992022143349  The environmental degradation depletes ecosyst...   \n",
       "2  992022143349  The environmental degradation depletes ecosyst...   \n",
       "3  992022143349  The environmental degradation depletes ecosyst...   \n",
       "4  912202292720  High-shear, low-CAPE (HSLC) severe convection ...   \n",
       "\n",
       "                                            question  \\\n",
       "0       What issue causes environmental degradation?   \n",
       "1                 What is Pollution is created from?   \n",
       "2                          What depletes ecosystems?   \n",
       "3                        What is a pollution source?   \n",
       "4  How would you characterize HSLC environments a...   \n",
       "\n",
       "                                         answer_text  answer_start  \\\n",
       "0                                          Pollution            51   \n",
       "1  vehicle emissions, agricultural runoff, chemic...           162   \n",
       "2                         Environmental degradation              4   \n",
       "3                               A variety of sources           130   \n",
       "4  Drier lower troposphere and a surface triple-p...           751   \n",
       "\n",
       "                                             answers  \n",
       "0      {'text': ['Pollution'], 'answer_start': [51]}  \n",
       "1  {'text': ['vehicle emissions, agricultural run...  \n",
       "2  {'text': ['Environmental degradation '], 'answ...  \n",
       "3  {'text': ['A variety of sources'], 'answer_sta...  \n",
       "4  {'text': ['Drier lower troposphere and a surfa...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f646ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qa_train, data_qa_test = train_test_split(data_qa, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcc17807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102, 6), (26, 6))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_qa_train.shape, data_qa_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3772352",
   "metadata": {},
   "source": [
    "### Preprocess for training\n",
    "\n",
    "- tokenization\n",
    "- chunking\n",
    "- etc\n",
    "\n",
    "References:\n",
    "- https://huggingface.co/docs/transformers/tasks/question_answering\n",
    "- https://github.com/AmitNikhade/Kaggle/blob/main/chaii%20-%20Hindi%20and%20Tamil%20Question%20Answering/question-answering-roberta-starter-explained.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f7000c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"data/nasawiki-v6/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/sq2-v6/train-watbertv6-squad-2ep/\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7c53121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sq2-v6/train-watbertv6-squad-2ep/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3251f495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 'right')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len_single_sentence, tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d68f483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65169c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, max_length=384, stride=128):\n",
    "#     questions = list(map(lambda x: x[\"question\"], examples))\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        sample_index = sample_mapping[i]\n",
    "\n",
    "        answer = answers[sample_index]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d8b6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_function(data_qa_train.iloc[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d524c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(data_qa_train)\n",
    "test_dataset = Dataset.from_pandas(data_qa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1981b24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 26)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "261fb2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56d93f993d344e4a1a4de66b763cb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_trains = train_dataset.map(\n",
    "    functools.partial(preprocess_function, tokenizer=tokenizer, max_length=384, stride=128),\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "986de173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5100c2eb58f5414e80c40d736e191c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_tests = test_dataset.map(\n",
    "    functools.partial(preprocess_function, tokenizer=tokenizer, max_length=384, stride=128),\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9e2e727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "     num_rows: 104\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "     num_rows: 26\n",
       " }))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_trains, tokenized_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8a36430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  5567,\n",
       "  375,\n",
       "  685,\n",
       "  1594,\n",
       "  1872,\n",
       "  285,\n",
       "  820,\n",
       "  335,\n",
       "  20624,\n",
       "  287,\n",
       "  1565,\n",
       "  3964,\n",
       "  285,\n",
       "  267,\n",
       "  2761,\n",
       "  35,\n",
       "  2,\n",
       "  2,\n",
       "  28082,\n",
       "  392,\n",
       "  5112,\n",
       "  3814,\n",
       "  287,\n",
       "  267,\n",
       "  1131,\n",
       "  1617,\n",
       "  16,\n",
       "  3262,\n",
       "  820,\n",
       "  292,\n",
       "  3433,\n",
       "  820,\n",
       "  375,\n",
       "  591,\n",
       "  1594,\n",
       "  1872,\n",
       "  285,\n",
       "  820,\n",
       "  335,\n",
       "  20624,\n",
       "  287,\n",
       "  1565,\n",
       "  3964,\n",
       "  285,\n",
       "  267,\n",
       "  2761,\n",
       "  18,\n",
       "  541,\n",
       "  465,\n",
       "  3745,\n",
       "  16,\n",
       "  731,\n",
       "  2438,\n",
       "  298,\n",
       "  975,\n",
       "  438,\n",
       "  339,\n",
       "  18,\n",
       "  2198,\n",
       "  13,\n",
       "  2608,\n",
       "  349,\n",
       "  31692,\n",
       "  3433,\n",
       "  820,\n",
       "  335,\n",
       "  20624,\n",
       "  5392,\n",
       "  16,\n",
       "  292,\n",
       "  2563,\n",
       "  2438,\n",
       "  298,\n",
       "  42736,\n",
       "  438,\n",
       "  339,\n",
       "  18,\n",
       "  2198,\n",
       "  13,\n",
       "  2608,\n",
       "  349,\n",
       "  3262,\n",
       "  820,\n",
       "  3456,\n",
       "  18,\n",
       "  541,\n",
       "  267,\n",
       "  1030,\n",
       "  377,\n",
       "  2183,\n",
       "  438,\n",
       "  339,\n",
       "  18,\n",
       "  298,\n",
       "  5783,\n",
       "  478,\n",
       "  267,\n",
       "  3967,\n",
       "  3748,\n",
       "  267,\n",
       "  624,\n",
       "  1293,\n",
       "  285,\n",
       "  31692,\n",
       "  3433,\n",
       "  820,\n",
       "  3456,\n",
       "  292,\n",
       "  8204,\n",
       "  285,\n",
       "  685,\n",
       "  31692,\n",
       "  21170,\n",
       "  287,\n",
       "  2817,\n",
       "  857,\n",
       "  267,\n",
       "  12034,\n",
       "  433,\n",
       "  34233,\n",
       "  1777,\n",
       "  1684,\n",
       "  299,\n",
       "  30969,\n",
       "  267,\n",
       "  2034,\n",
       "  285,\n",
       "  3433,\n",
       "  820,\n",
       "  3456,\n",
       "  292,\n",
       "  8204,\n",
       "  879,\n",
       "  1586,\n",
       "  1355,\n",
       "  292,\n",
       "  4366,\n",
       "  5622,\n",
       "  18,\n",
       "  3844,\n",
       "  11364,\n",
       "  287,\n",
       "  779,\n",
       "  1030,\n",
       "  362,\n",
       "  1586,\n",
       "  1355,\n",
       "  16,\n",
       "  3245,\n",
       "  267,\n",
       "  5346,\n",
       "  1050,\n",
       "  397,\n",
       "  3779,\n",
       "  335,\n",
       "  3433,\n",
       "  4394,\n",
       "  7004,\n",
       "  16,\n",
       "  1224,\n",
       "  6442,\n",
       "  5622,\n",
       "  972,\n",
       "  355,\n",
       "  10142,\n",
       "  14613,\n",
       "  292,\n",
       "  37325,\n",
       "  295,\n",
       "  16,\n",
       "  12151,\n",
       "  292,\n",
       "  23836,\n",
       "  536,\n",
       "  3779,\n",
       "  335,\n",
       "  3433,\n",
       "  820,\n",
       "  3456,\n",
       "  39798,\n",
       "  18,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'start_positions': 28,\n",
       " 'end_positions': 32}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_trains[0][\"input_ids\"][\"start_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "78ccb21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>What are two important sources of water for drinking in many parts of the world?</s></s>Apart from groundwater mentioned in the above studies, river water and spring water are also important sources of water for drinking in many parts of the world. In this issue, one paper (He et al. 2019) focus on karst spring water for drinking purpose, and another paper (Ahmed et al. 2019) focus on river water quality. In the study by He et al. (2019), the authors analyzed the time series of karst spring water quality and quantity of two karst springs in China using the Mannâ€“Kendall trend test to delineate the variations of spring water quality and quantity under climate change and human activities. They proved in their study that climate change, particularly the decreasing precipitation was responsible for spring discharge attenuation, while anthropogenic activities such as coal mining and quarrying, agriculture and urbanization were responsible for spring water quality deterioration.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_trains[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a7806b",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42328c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98e8cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, default_data_collator\n",
    "from transformers import AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "019f6315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e63d87a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at data/sq2-v6/train-watbertv6-squad-2ep/ were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'task_heads.qa_head.classifier.dense.bias', 'task_heads.qa_head.qa_outputs.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'task_heads.qa_head.classifier.out_proj.bias', 'task_heads.qa_head.classifier.out_proj.weight', 'task_heads.qa_head.classifier.dense.weight', 'lm_head.dense.weight', 'task_heads.qa_head.qa_outputs.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at data/sq2-v6/train-watbertv6-squad-2ep/ and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"data/sq2-v6/train-watbertv6-squad-2ep/\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"data/nasawiki-v6/\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9beeb123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(65536, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdafc3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train-watbertv6-squad-2ep'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path(model.name_or_path).stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1407f6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnishuah\u001b[0m (\u001b[33mnish-test\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1a27b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e5bd25f05441238cb75fc2c4d8bd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0167530395833334, max=1.0))â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nishparadox/dev/uah/nasa-impact/llm-experiments/nasa_wiki_v6/wandb/run-20230112_111114-22gnul02</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nish-test/llm-test/runs/22gnul02\" target=\"_blank\">firm-sun-37</a></strong> to <a href=\"https://wandb.ai/nish-test/llm-test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/nish-test/llm-test/runs/22gnul02?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2ebbf7e50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"llm-test\",\n",
    "    entity=\"nish-test\",\n",
    "    tags=[\"qa\", pathlib.Path(model.name_or_path).stem]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21ec76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    f\"tmp/finetuned/qa/{pathlib.Path(model.name_or_path).stem}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "91f8b0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp/finetuned/qa/train-watbertv6-squad-2ep'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba2abdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=tokenized_trains,\n",
    "    eval_dataset=tokenized_tests,\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed9ec1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishparadox/dev/uah/nasa-impact/llm-experiments/nasa_wiki_v6/venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 104\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 45\n",
      "  Number of trainable parameters = 135783938\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 16:40, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.785500</td>\n",
       "      <td>5.599750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.541900</td>\n",
       "      <td>3.937743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.042500</td>\n",
       "      <td>2.802520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.881400</td>\n",
       "      <td>2.412601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.434300</td>\n",
       "      <td>2.389620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.179700</td>\n",
       "      <td>2.551411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.073400</td>\n",
       "      <td>2.713697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>2.769324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.514100</td>\n",
       "      <td>2.771827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.579600</td>\n",
       "      <td>2.788648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.720500</td>\n",
       "      <td>2.763725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.667900</td>\n",
       "      <td>2.730632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.532500</td>\n",
       "      <td>2.721690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>2.733320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.419500</td>\n",
       "      <td>2.741311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-3\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-3/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-3/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-3/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-3/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-27] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-6\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-6/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-6/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-6/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-6/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-9\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-9/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-9/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-9/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-9/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-3] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-12\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-12/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-12/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-12/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-12/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-6] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-15\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-15/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-15/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-15/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-15/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-9] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-18\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-18/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-18/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-18/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-18/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-12] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-21\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-21/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-21/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-21/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-21/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-15] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-24\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-24/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-24/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-24/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-24/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-18] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-27\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-27/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-27/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-27/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-27/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-21] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-30\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-30/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-30/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-24] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-33\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-33/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-33/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-33/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-33/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-27] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-36\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-36/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-36/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-36/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-36/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-39/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-39/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-39/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-39/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-33] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-42\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-42/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-42/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-42/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-42/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-36] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45\n",
      "Configuration saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/config.json\n",
      "Model weights saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/tokenizer_config.json\n",
      "Special tokens file saved in tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/special_tokens_map.json\n",
      "Deleting older checkpoint [tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-39] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45, training_loss=1.8422791805532244, metrics={'train_runtime': 1016.1629, 'train_samples_per_second': 1.535, 'train_steps_per_second': 0.044, 'total_flos': 304149424840704.0, 'train_loss': 1.8422791805532244, 'epoch': 14.92})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e39078",
   "metadata": {},
   "source": [
    "## predict/evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19e2dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e85e73b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jury import Jury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f376181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla -> distil bert\n",
    "qa_pipe = pipeline(\n",
    "    \"question-answering\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "389189b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"decoding_times_with_dropout\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_dropout_rate\": 0.25,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65536\n",
      "}\n",
      "\n",
      "loading configuration file tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"decoding_times_with_dropout\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_dropout_rate\": 0.25,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65536\n",
      "}\n",
      "\n",
      "loading weights file tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "qa_pipe = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/\",\n",
    "    tokenizer=\"tmp/finetuned/qa/train-watbertv6-squad-2ep/checkpoint-45/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2129dd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(65536, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a41ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7ef28c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = qa_pipe(\n",
    "    list(map(lambda x: dict(context=x[\"context\"], question=x[\"question\"]), data_qa_test.to_dict(\"records\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "264dfc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "65108612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3066351115703583, 'start': 242, 'end': 246, 'answer': 'Dust'},\n",
       " {'score': 0.9677391648292542,\n",
       "  'start': 101,\n",
       "  'end': 110,\n",
       "  'answer': 'resources'},\n",
       " {'score': 0.16440102458000183, 'start': 82, 'end': 91, 'answer': 'bands 3â€“7'},\n",
       " {'score': 0.08084839582443237,\n",
       "  'start': 907,\n",
       "  'end': 995,\n",
       "  'answer': 'satellite scatterometers have been providing valuable sea surface wind field information'},\n",
       " {'score': 0.00781230628490448, 'start': 800, 'end': 801, 'answer': '.'},\n",
       " {'score': 0.04603151977062225,\n",
       "  'start': 583,\n",
       "  'end': 652,\n",
       "  'answer': 'dust in the SAL can be drawn into nearby mesoscale convective systems'},\n",
       " {'score': 0.006601804867386818,\n",
       "  'start': 519,\n",
       "  'end': 540,\n",
       "  'answer': 'In all global regions'},\n",
       " {'score': 0.7251588106155396,\n",
       "  'start': 583,\n",
       "  'end': 608,\n",
       "  'answer': 'half the radar wavelength'},\n",
       " {'score': 0.5100980401039124,\n",
       "  'start': 200,\n",
       "  'end': 242,\n",
       "  'answer': '175 km east of Jackson, Mississippi (JAN).'},\n",
       " {'score': 0.44714125990867615,\n",
       "  'start': 874,\n",
       "  'end': 888,\n",
       "  'answer': 'Aprilâ€“August).'},\n",
       " {'score': 0.6092959046363831, 'start': 313, 'end': 319, 'answer': 'humans'},\n",
       " {'score': 0.9392579793930054,\n",
       "  'start': 0,\n",
       "  'end': 14,\n",
       "  'answer': 'Wildland fires'},\n",
       " {'score': 0.4065626859664917, 'start': 476, 'end': 482, 'answer': 'forest'},\n",
       " {'score': 0.6854976415634155,\n",
       "  'start': 749,\n",
       "  'end': 818,\n",
       "  'answer': 'a drier lower troposphere and a surface triple-point or upslope setup'},\n",
       " {'score': 0.5809596180915833,\n",
       "  'start': 200,\n",
       "  'end': 217,\n",
       "  'answer': 'central US region'},\n",
       " {'score': 0.6240906715393066,\n",
       "  'start': 571,\n",
       "  'end': 629,\n",
       "  'answer': 'in situ monitoring, remote sensing, and numerical modeling'},\n",
       " {'score': 0.9109280109405518,\n",
       "  'start': 172,\n",
       "  'end': 188,\n",
       "  'answer': 'Bragg scattering'},\n",
       " {'score': 0.014791046269237995, 'start': 11, 'end': 13, 'answer': '0.'},\n",
       " {'score': 0.9567453861236572, 'start': 676, 'end': 680, 'answer': 'VOCs'},\n",
       " {'score': 0.32669058442115784,\n",
       "  'start': 165,\n",
       "  'end': 212,\n",
       "  'answer': 'they tend to act as a trigger for thunderstorms'},\n",
       " {'score': 0.13388869166374207,\n",
       "  'start': 358,\n",
       "  'end': 367,\n",
       "  'answer': 'satellite'},\n",
       " {'score': 0.11640525609254837,\n",
       "  'start': 403,\n",
       "  'end': 488,\n",
       "  'answer': 'changes to the hurricane boundary layer as the storm moved inland and began weakening'},\n",
       " {'score': 0.9699695110321045,\n",
       "  'start': 640,\n",
       "  'end': 650,\n",
       "  'answer': 'year-round'},\n",
       " {'score': 0.5093109011650085,\n",
       "  'start': 394,\n",
       "  'end': 403,\n",
       "  'answer': 'Dm and Ze'},\n",
       " {'score': 0.8027727603912354,\n",
       "  'start': 914,\n",
       "  'end': 977,\n",
       "  'answer': 'characterizes the strength of the refractive index fluctuations'},\n",
       " {'score': 4.451155837159604e-05,\n",
       "  'start': 836,\n",
       "  'end': 887,\n",
       "  'answer': 'D/ND consisted of 59% of the area of deforestation.'}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "335faa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('It is estimated that 240 Â± 80 million tons of Saharan dust are transported from Africa to the Atlantic Ocean every year [Kaufman et al., 2005]. Dust can directly impact the radiative balance of the earth by absorbing and scattering sunlight. Dust, especially those coated with sulfur and other soluble materials [Levin et al., 1996], can also affect the cloud development by acting as cloud condensation nuclei (CCN), giant CCN (GCCN) and ice nuclei (IN) [DeMott et al., 2003; van den Heever et al., 2006]. Analysis of visible satellite imagery over the east Atlantic shows that the dust in the SAL can be drawn into nearby mesoscale convective systems, which can later develop into TCs. Hurricane Erin (2001) was one example. Although previous studies have shown that large numbers of dust particles can impact the development of shallow convective clouds, the role of CCN on the development of highly organized, strong convective systems such as TCs has not been thoroughly investigated. This study will examine the impact of dust acting as CCN on the initiation and evolution of a TC using numerical simulations.',\n",
       " 'How does does impact cloud formation?',\n",
       " 'Dust, especially those coated with sulfur and other soluble materials [Levin et al., 1996], can also affect the cloud development by acting as cloud condensation nuclei (CCN), giant CCN (GCCN) and ice nuclei (IN) [DeMott et al., 2003; van den Heever et al., 2006]',\n",
       " {'score': 0.3066351115703583, 'start': 242, 'end': 246, 'answer': 'Dust'})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(data_qa_test[\"context\"], data_qa_test[\"question\"], data_qa_test[\"answer_text\"], predictions))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2c00628c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_fuzzy(gts: List[str], predictions: List[str]):\n",
    "    _preprocess = lambda x: x.strip(\" .,!?\").lower()\n",
    "    gts = list(map(_preprocess, gts))\n",
    "    predictions = list(map(_preprocess, predictions))\n",
    "    res = []\n",
    "    for gt, pred in zip(gts, predictions):\n",
    "        res.append(fuzz.token_set_ratio(gt, pred))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dd6ff935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_exact(gts: List[str], predictions: List[str]):\n",
    "    _preprocess = lambda x: x.strip(\" .,!?\").lower()\n",
    "    gts = list(map(_preprocess, gts))\n",
    "    predictions = list(map(_preprocess, predictions))\n",
    "    res = []\n",
    "    for gt, pred in zip(gts, predictions):\n",
    "        res.append((gt == pred)*100)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "42ab2353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.56862745098039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ground truth vs prediction exact match')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGzCAYAAAAFROyYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtDElEQVR4nO3deXgUVb7/8U93Ap0EsrBIFo0YAyOrRlliWEeMZgKDIiAwoBMWBRUFjBckKmFRJoN4JaIgLjOAXHHhqrg8LGJA0WERQZhRFEGjopggCEkIECA5vz/4pS5NAgTs2Cf4fj1PP0/6VNXpb1VXpz99uqraZYwxAgAAsIjb3wUAAACcjIACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgILfFZfLpUmTJvm7jFO65JJL9Oc//9nfZVjt5Odw3rx5crlc+vbbb33S/7fffiuXy6V58+b5pD/YjdecvQgoqCA3N1d33323/vCHPygkJEQhISFq0aKFRo4cqX//+9/+Lq/a7dq1S5MmTdLmzZurpf+tW7dq0qRJPntDxblZuHChsrOz/V1GjbFkyRIrw/2aNWs0adIk7d+/39+lwMcIKPDyzjvvqFWrVlqwYIGSk5M1Y8YMPfHEE0pNTdWSJUuUkJCg7777zt9lVqtdu3Zp8uTJ1RpQJk+eTEDxkVtvvVWHDh1S48aNz2q5UwWUxo0b69ChQ7r11lt9VOH5YcmSJZo8ebK/y6hgzZo1mjx5MgHlPBTo7wJgj6+//loDBgxQ48aNlZOTo+joaK/p06ZN0+zZs+V2nz7XFhcXq06dOtVZqlUOHjyokJAQf5dhveraTgEBAQoICPBZfy6XS0FBQT7rD8A5MsD/N3z4cCPJrFu3rsrLpKWlmTp16pgdO3aY1NRUU7duXXPjjTcaY4w5cOCASU9PNxdddJGpXbu2+cMf/mCmT59uysrKnOVzc3ONJDN37twKfUsyEydOdO5PnDjRSDLbt283aWlpJjw83ISFhZnBgweb4uJir2UPHz5sxowZYxo2bGjq1q1revbsaXbu3Fmhz5OtWrXKSKpwK6+va9eupmXLluaTTz4xnTt3NsHBwWb06NGV1luucePGJi0tzRhjzNy5cyvtf9WqVc68PXr0MB9++KFp166d8Xg8Ji4uzsyfP/90T4M5cuSIqVevnhk8eHCFaQUFBcbj8Zj77rvPaZs5c6Zp0aKFCQ4ONhEREaZNmzbmxRdfPO1jlG+bl19+2WRkZJjIyEgTEhJievbsab7//nuveU+3nQ4fPmwyMzNNfHy8qV27trnooovM2LFjzeHDh736qOpzWL5Nc3NzvZZfsmSJ6dKli6lbt64JDQ01bdu2ddaxa9euFZ6Dxo0bG2NOvU/m5OSYTp06mZCQEBMeHm5uuOEGs3XrVq95zmYfPZV169aZlJQUExYWZoKDg02XLl3MRx995EzfunWrCQoKMrfeeqvXch9++KFxu91m3LhxTtvixYtN9+7dTXR0tKldu7a59NJLzZQpU8yxY8cqfdzU1FQTERFhQkJCTOvWrU12drYx5vjrvLL99nTK9+VVq1aZNm3amKCgINOqVStnX3/ttddMq1atjMfjMVdddZXZtGmT1/JbtmwxaWlpJi4uzng8HhMZGWmGDBli9uzZU2F7n3w7cV9YsGCBadeunbOvd+7c2SxfvrxCnWf7mkP1YwQFjnfeeUdNmjRRYmLiWS137NgxpaSkqFOnTnrssccUEhIiY4xuuOEGrVq1SsOGDVNCQoKWL1+usWPH6scff9SMGTPOuc5+/fopLi5OWVlZ2rRpk55//nk1atRI06ZNc+a57bbb9D//8z8aOHCgOnTooJUrV6pHjx5n7Lt58+aaMmWKMjMzNXz4cHXu3FmS1KFDB2eevXv3KjU1VQMGDNAtt9yiyMjIKtfepUsXjRo1SjNnztQDDzyg5s2bO49bbseOHerbt6+GDRumtLQ0/fOf/9TgwYPVpk0btWzZstJ+a9WqpZtuukmvv/66nnnmGdWuXduZtnjxYpWUlGjAgAGSpOeee06jRo1S3759NXr0aB0+fFj//ve/tX79eg0cOPCM6zB16lS5XC7df//92r17t7Kzs5WcnKzNmzcrODj4tNuprKxMN9xwgz766CMNHz5czZs313/+8x/NmDFDX331lRYvXuwsf67PoXT8wNmhQ4eqZcuWysjIUEREhD799FMtW7ZMAwcO1IMPPqiCggL98MMPzr5Yt27dU/b33nvvKTU1VZdeeqkmTZqkQ4cO6cknn1THjh21adMmXXLJJV7zV2UfrczKlSuVmpqqNm3aaOLEiXK73Zo7d666deumDz/8UO3bt1fz5s318MMPa+zYserbt69uuOEGFRcXa/DgwWrWrJmmTJnitR3q1q2r9PR01a1bVytXrlRmZqYKCws1ffp0Z74VK1boz3/+s6KjozV69GhFRUXpiy++0DvvvKPRo0drxIgR2rVrl1asWKEFCxZU6TmQju/LAwcO1IgRI3TLLbfoscceU8+ePTVnzhw98MADuuuuuyRJWVlZ6tevn7Zt2+aM0K5YsULffPONhgwZoqioKH3++ed69tln9fnnn2vdunVyuVzq3bu3vvrqK7300kuaMWOGGjZsKEm64IILJEmTJ0/WpEmT1KFDB02ZMkW1a9fW+vXrtXLlSl1//fVedZ7taw6/AX8nJNihoKDASDK9evWqMG3fvn3m559/dm4HDx50ppV/sho/frzXMosXLzaSzCOPPOLV3rdvX+NyucyOHTuMMec2gjJ06FCv+W666SbToEED5/7mzZuNJHPXXXd5zTdw4MAzjqAYY8yGDRtOWVP5J+85c+acsd5yJ46gGGPMokWLvEZNTp5Xklm9erXTtnv37gojIJVZvny5kWTefvttr/bu3bubSy+91Ll/4403mpYtW562r8qUj6BceOGFprCw0Gl/9dVXjSTzxBNPOG2n2k4LFiwwbrfbfPjhh17tc+bMMZLMv/71L2PM2T2HJ4+g7N+/34SGhprExERz6NAhr+VPHL3r0aOHM2pyosr2yYSEBNOoUSOzd+9ep23Lli3G7Xabv/71r05bVffRypSVlZmmTZualJQUrzoPHjxo4uLizHXXXee0lZaWmk6dOpnIyEizZ88eM3LkSBMYGGg2bNjg1eeJr9VyI0aMMCEhIc6I1bFjx0xcXJxp3Lix2bdvX4Wayo0cOfKMoyYnKt+X16xZ47SV76PBwcHmu+++c9qfeeaZCq+Jymp/6aWXKrw+pk+fXukI2vbt243b7TY33XSTKS0tPeV6/ZrXHKoXB8lCklRYWCip8k+Rf/zjH3XBBRc4t1mzZlWY58477/S6v2TJEgUEBGjUqFFe7ffdd5+MMVq6dOk513rHHXd43e/cubP27t3rrMOSJUskqcJjjxkz5pwf80Qej0dDhgzxSV+VadGihTNyIx3/NHjZZZfpm2++Oe1y3bp1U8OGDfXKK684bfv27dOKFSvUv39/py0iIkI//PCDNmzYcE71/fWvf1VoaKhzv2/fvoqOjna2e7nKttOiRYvUvHlzNWvWTHv27HFu3bp1kyStWrVK0q97DlesWKGioiKNHz++wrEkLperait5gp9++kmbN2/W4MGDVb9+faf98ssv13XXXVdhvaUz76OV2bx5s7Zv366BAwdq7969zrYpLi7Wtddeq9WrV6usrEyS5Ha7NW/ePB04cECpqamaPXu2MjIy1LZtW68+TxzRKioq0p49e9S5c2cdPHhQX375pSTp008/VW5ursaMGaOIiAiv5c9le52oRYsWSkpKcu6Xj85269ZNF198cYX2E/fxE2s/fPiw9uzZo6uvvlqStGnTpjM+9uLFi1VWVqbMzMwKx82dvF7n+ppD9eIrHkiS84Zz4MCBCtOeeeYZFRUVKT8/X7fcckuF6YGBgbrooou82r777jvFxMR4vZFJ//dVxq85E+jEf2ySVK9ePUnH34zDwsL03Xffye12Kz4+3mu+yy677Jwf80QXXnih11covnby+knH13Hfvn2nXS4wMFB9+vTRwoULVVJSIo/Ho9dff11Hjx71Cij333+/3nvvPbVv315NmjTR9ddfr4EDB6pjx45Vqq9p06Ze910ul5o0aVLhrKTKttP27dv1xRdfOEPwJ9u9e7ck/arn8Ouvv5YktWrV6ozzVkX5vlrZYzdv3lzLly+vcGD4mfbRymzfvl2SlJaWdspaCgoKnL7i4+M1adIkjR07Vq1atdKECRMqzP/555/roYce0sqVKyuEo4KCAkm+314nOnk7hIeHS5JiY2MrbT9xH//ll180efJkvfzyy85+Ua689tP5+uuv5Xa71aJFi7OuU6raaw7Vi4ACScf/QURHR+uzzz6rMK38082pTov1eDxnPLPnVE71Ca20tPSUy5zqjA1jzDnVcLZO/GRXFadbl8r8mvUbMGCAnnnmGS1dulS9evXSq6++qmbNmumKK65w5mnevLm2bdumd955R8uWLdNrr72m2bNnKzMz06enkVa2ncrKytS6dWs9/vjjlS5z8htXTXUuz2H56Mj06dOVkJBQ6Twnj3C+++67ko6fGr93715FRUU50/bv36+uXbsqLCxMU6ZMUXx8vIKCgrRp0ybdf//9zuNVp1Nth6psn379+mnNmjUaO3asEhISVLduXZWVlelPf/qTz2v39/8UVI6AAkePHj30/PPP6+OPP1b79u1/VV+NGzfWe++9p6KiIq9RlPJh5fJrVpR/Gjz5Gga/ZoSlcePGKisr09dff+31qXfbtm1VWv5ch7Xr1atXYT2OHDmin376ySf9V0WXLl0UHR2tV155RZ06ddLKlSv14IMPVpivTp066t+/v/r3768jR46od+/emjp1qjIyMs54im35J/1yxhjt2LFDl19++Rnri4+P15YtW3Tttdeedjv8muewfNTls88+U5MmTU45X1Wfh/J9tbLH/vLLL9WwYUOfnFZfXndYWJiSk5PPOP+cOXO0YsUKTZ06VVlZWRoxYoTefPNNZ/r777+vvXv36vXXX1eXLl2c9tzc3Eof97PPPjvt41bnfnuyffv2KScnR5MnT1ZmZqbTfvK+d7q64uPjVVZWpq1bt54y8MFuHIMCx7hx4xQSEqKhQ4cqPz+/wvSz+TTRvXt3lZaW6qmnnvJqnzFjhlwul1JTUyUd/2fcsGFDrV692mu+2bNnn8MaHFfe98yZM73aq3rV0PI3m7O98FN8fHyF9Xj22WcrjKCca/9V4Xa71bdvX7399ttasGCBjh075vX1jnT87JoT1a5dWy1atJAxRkePHj3jY7zwwgsqKipy7v/v//6vfvrpJ2e7n06/fv30448/6rnnnqsw7dChQyouLpb0657D66+/XqGhocrKytLhw4e9pp24D9epU6dKXxVER0crISFB8+fP93rOPvvsM7377rvq3r37GfuoijZt2ig+Pl6PPfZYpV+1/vzzz87fubm5Gjt2rPr06aMHHnhAjz32mN566y298MILzjzlowInrvORI0cqvLauuuoqxcXFKTs7u8I+efL2kqpnvz1ZZbVLlT//p6qrV69ecrvdmjJlSoURF0ZGagZGUOBo2rSpFi5cqL/85S+67LLLNGjQIF1xxRUyxig3N1cLFy6U2+2ucLxJZXr27KlrrrlGDz74oL799ltdccUVevfdd/Xmm29qzJgxXscW3Hbbbfr73/+u2267TW3bttXq1av11VdfnfN6JCQk6C9/+Ytmz56tgoICdejQQTk5OdqxY0eVlo+Pj1dERITmzJmj0NBQ1alTR4mJiYqLizvtcrfddpvuuOMO9enTR9ddd522bNmi5cuXO6c+nlhfQECApk2bpoKCAnk8HnXr1k2NGjU653U+Uf/+/fXkk09q4sSJat26tdcpzNLxN/CoqCh17NhRkZGR+uKLL/TUU0+pR48eFY4Zqkz9+vXVqVMnDRkyRPn5+crOzlaTJk10++23n3HZW2+9Va+++qruuOMOrVq1Sh07dlRpaam+/PJLvfrqq1q+fLnatm37q57DsLAwzZgxQ7fddpvatWungQMHql69etqyZYsOHjyo+fPnSzoeCF555RWlp6erXbt2qlu3rnr27Flpn9OnT1dqaqqSkpI0bNgw5zTj8PBwn13+3e126/nnn1dqaqpatmypIUOG6MILL9SPP/6oVatWKSwsTG+//baMMRo6dKiCg4P19NNPS5JGjBih1157TaNHj1ZycrJiYmLUoUMH1atXT2lpaRo1apRcLpcWLFhQ4c3Z7Xbr6aefVs+ePZWQkKAhQ4YoOjpaX375pT7//HMtX77c2V7S8QOXU1JSFBAQ4Jy67mthYWHq0qWLHn30UR09elQXXnih3n333QqjPyfW9eCDD2rAgAGqVauWevbsqSZNmujBBx/Uww8/rM6dO6t3797yeDzasGGDYmJilJWVVS21w4d+8/OGYL0dO3aYO++80zRp0sQEBQWZ4OBg06xZM3PHHXeYzZs3e81bfqG2yhQVFZl7773XxMTEmFq1apmmTZtWuFCbMcdPJxw2bJgJDw83oaGhpl+/fmb37t2nPM34559/9lq+sgt1HTp0yIwaNco0aNDA1KlTp8oXaiv35ptvmhYtWpjAwMBKL9RWmdLSUnP//febhg0bmpCQEJOSkmJ27NhR4TRjY4x57rnnzKWXXmoCAgIqvVDbybp27Wq6du16xrqNOX4KZWxsbKWneRtz/JTOLl26mAYNGhiPx2Pi4+PN2LFjTUFBwWn7LT/N+KWXXjIZGRmmUaNGJjg42PTo0cPrlNHyek+1nY4cOWKmTZtmWrZsaTwej6lXr55p06aNmTx5slcNVX0OT3Whtrfeest06NDBBAcHm7CwMNO+fXvz0ksvOdMPHDhgBg4caCIiIqp0obb33nvPdOzY0emvZ8+ep7xQW1X20VP59NNPTe/evZ3np3HjxqZfv34mJyfHGGPME088YSSZ1157zWu577//3oSFhZnu3bs7bf/617/M1VdfbYKDg01MTIwZN26cc6rvyae5f/TRR+a6664zoaGhpk6dOubyyy83Tz75pDP92LFj5p577jEXXHCBcblcVb5Q28kkmZEjR3q1lW/z6dOnO20//PCDuemmm0xERIQJDw83N998s9m1a1elr+GHH37YXHjhhcbtdlfYzv/85z/NlVde6exrXbt2NStWrDhjnWfzmkP1cBnDWBeAM3v//fd1zTXXaNGiRerbt6+/ywFwnuMYFAAAYB0CCgAAsA4BBQAAWIdjUAAAgHUYQQEAANYhoAAAAOvUyAu1lZWVadeuXQoNDf1NL78MAADOnTFGRUVFiomJOeNvuNXIgLJr167z5kfFAAD4vdm5c+cZr0peIwNK+eW4d+7cecqfLgcAAHYpLCxUbGxslX5Wo0YGlPKvdcLCwggoAADUMFU5PIODZAEAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOmcdUFavXq2ePXsqJiZGLpdLixcv9ppujFFmZqaio6MVHBys5ORkbd++3WueX375RYMGDVJYWJgiIiI0bNgwHThw4FetCAAAOH+c9W/xFBcX64orrtDQoUPVu3fvCtMfffRRzZw5U/Pnz1dcXJwmTJiglJQUbd26VUFBQZKkQYMG6aefftKKFSt09OhRDRkyRMOHD9fChQt//RrVEGVlZSooKPB3GdDxUF1SUuLvMgBreTyeKv12CqpfeHi43O7fx5cfLmOMOeeFXS698cYb6tWrl6Tj/+hjYmJ033336b/+678kSQUFBYqMjNS8efM0YMAAffHFF2rRooU2bNigtm3bSpKWLVum7t2764cfflBMTEyFxykpKfF6Ayn/NcSCgoIa+2OB+/bt00033eTvMgAANcgbb7yhevXq+buMc1ZYWKjw8PAqvX/7NIbl5uYqLy9PycnJTlt4eLgSExO1du1aSdLatWsVERHhhBNJSk5Oltvt1vr16yvtNysrS+Hh4c4tNjbWl2UDAADLnPVXPKeTl5cnSYqMjPRqj4yMdKbl5eWpUaNG3kUEBqp+/frOPCfLyMhQenq6c798BKUm83g8zt8HWt8s4w7wYzW/c0ZS2TF/VwHYyx0o8Q2P37jKSlX3P4skeb93nO98GlCqi8fjOe+elBO/zzW1gqSAWn6sBgBgK1N61Pn793QskE+/4omKipIk5efne7Xn5+c706KiorR7926v6ceOHdMvv/zizAMAAH7ffBpQ4uLiFBUVpZycHKetsLBQ69evV1JSkiQpKSlJ+/fv18aNG515Vq5cqbKyMiUmJvqyHAAAUEOd9Vc8Bw4c0I4dO5z7ubm52rx5s+rXr6+LL75YY8aM0SOPPKKmTZs6pxnHxMQ4Z/o0b95cf/rTn3T77bdrzpw5Onr0qO6++24NGDCg0jN4AADA789ZB5RPPvlE11xzjXO//ODVtLQ0zZs3T+PGjVNxcbGGDx+u/fv3q1OnTlq2bJlzDRRJevHFF3X33Xfr2muvldvtVp8+fTRz5kwfrA4AADgf/KrroPjL2ZxHbatDhw4pNTVVklR01a0cJAsAqFzpUYVuWiBJWrp0qYKDg/1c0Lnz23VQAAAAfIGAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwjs8DSmlpqSZMmKC4uDgFBwcrPj5eDz/8sIwxzjzGGGVmZio6OlrBwcFKTk7W9u3bfV0KAACooXweUKZNm6ann35aTz31lL744gtNmzZNjz76qJ588klnnkcffVQzZ87UnDlztH79etWpU0cpKSk6fPiwr8sBAAA1UKCvO1yzZo1uvPFG9ejRQ5J0ySWX6KWXXtLHH38s6fjoSXZ2th566CHdeOONkqQXXnhBkZGRWrx4sQYMGODrkgAAQA3j8xGUDh06KCcnR1999ZUkacuWLfroo4+UmpoqScrNzVVeXp6Sk5OdZcLDw5WYmKi1a9dW2mdJSYkKCwu9bgAA4Pzl8xGU8ePHq7CwUM2aNVNAQIBKS0s1depUDRo0SJKUl5cnSYqMjPRaLjIy0pl2sqysLE2ePNnXpQIAAEv5fATl1Vdf1YsvvqiFCxdq06ZNmj9/vh577DHNnz//nPvMyMhQQUGBc9u5c6cPKwYAALbx+QjK2LFjNX78eOdYktatW+u7775TVlaW0tLSFBUVJUnKz89XdHS0s1x+fr4SEhIq7dPj8cjj8fi6VAAAYCmfj6AcPHhQbrd3twEBASorK5MkxcXFKSoqSjk5Oc70wsJCrV+/XklJSb4uBwAA1EA+H0Hp2bOnpk6dqosvvlgtW7bUp59+qscff1xDhw6VJLlcLo0ZM0aPPPKImjZtqri4OE2YMEExMTHq1auXr8sBAAA1kM8DypNPPqkJEyborrvu0u7duxUTE6MRI0YoMzPTmWfcuHEqLi7W8OHDtX//fnXq1EnLli1TUFCQr8sBAAA1kMuceInXGqKwsFDh4eEqKChQWFiYv8s5J4cOHXJOvS666lYpoJafKwIAWKn0qEI3LZAkLV26VMHBwX4u6Nydzfs3v8UDAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFinWgLKjz/+qFtuuUUNGjRQcHCwWrdurU8++cSZboxRZmamoqOjFRwcrOTkZG3fvr06SgEAADWQzwPKvn371LFjR9WqVUtLly7V1q1b9d///d+qV6+eM8+jjz6qmTNnas6cOVq/fr3q1KmjlJQUHT582NflAACAGijQ1x1OmzZNsbGxmjt3rtMWFxfn/G2MUXZ2th566CHdeOONkqQXXnhBkZGRWrx4sQYMGODrkgAAQA3j8xGUt956S23bttXNN9+sRo0a6corr9Rzzz3nTM/NzVVeXp6Sk5OdtvDwcCUmJmrt2rWV9llSUqLCwkKvGwAAOH/5PKB88803evrpp9W0aVMtX75cd955p0aNGqX58+dLkvLy8iRJkZGRXstFRkY6006WlZWl8PBw5xYbG+vrsgEAgEV8HlDKysp01VVX6W9/+5uuvPJKDR8+XLfffrvmzJlzzn1mZGSooKDAue3cudOHFQMAANv4PKBER0erRYsWXm3NmzfX999/L0mKioqSJOXn53vNk5+f70w7mcfjUVhYmNcNAACcv3weUDp27Kht27Z5tX311Vdq3LixpOMHzEZFRSknJ8eZXlhYqPXr1yspKcnX5QAAgBrI52fx3HvvverQoYP+9re/qV+/fvr444/17LPP6tlnn5UkuVwujRkzRo888oiaNm2quLg4TZgwQTExMerVq5evywEAADWQzwNKu3bt9MYbbygjI0NTpkxRXFycsrOzNWjQIGeecePGqbi4WMOHD9f+/fvVqVMnLVu2TEFBQb4uBwAA1EAuY4zxdxFnq7CwUOHh4SooKKixx6McOnRIqampkqSiq26VAmr5uSIAgJVKjyp00wJJ0tKlSxUcHOzngs7d2bx/81s8AADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1qj2g/P3vf5fL5dKYMWOctsOHD2vkyJFq0KCB6tatqz59+ig/P7+6SwEAADVEtQaUDRs26JlnntHll1/u1X7vvffq7bff1qJFi/TBBx9o165d6t27d3WWAgAAapBqCygHDhzQoEGD9Nxzz6levXpOe0FBgf7xj3/o8ccfV7du3dSmTRvNnTtXa9as0bp166qrHAAAUINUW0AZOXKkevTooeTkZK/2jRs36ujRo17tzZo108UXX6y1a9dW2ldJSYkKCwu9bgAA4PwVWB2dvvzyy9q0aZM2bNhQYVpeXp5q166tiIgIr/bIyEjl5eVV2l9WVpYmT55cHaUCAAAL+XwEZefOnRo9erRefPFFBQUF+aTPjIwMFRQUOLedO3f6pF8AAGAnnweUjRs3avfu3brqqqsUGBiowMBAffDBB5o5c6YCAwMVGRmpI0eOaP/+/V7L5efnKyoqqtI+PR6PwsLCvG4AAOD85fOveK699lr95z//8WobMmSImjVrpvvvv1+xsbGqVauWcnJy1KdPH0nStm3b9P333yspKcnX5QAAgBrI5wElNDRUrVq18mqrU6eOGjRo4LQPGzZM6enpql+/vsLCwnTPPfcoKSlJV199ta/LAQAANVC1HCR7JjNmzJDb7VafPn1UUlKilJQUzZ492x+lAAAAC/0mAeX999/3uh8UFKRZs2Zp1qxZv8XDAwCAGobf4gEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArOPzgJKVlaV27dopNDRUjRo1Uq9evbRt2zaveQ4fPqyRI0eqQYMGqlu3rvr06aP8/HxflwIAAGoonweUDz74QCNHjtS6deu0YsUKHT16VNdff72Ki4udee699169/fbbWrRokT744APt2rVLvXv39nUpAACghgr0dYfLli3zuj9v3jw1atRIGzduVJcuXVRQUKB//OMfWrhwobp16yZJmjt3rpo3b65169bp6quv9nVJAACghqn2Y1AKCgokSfXr15ckbdy4UUePHlVycrIzT7NmzXTxxRdr7dq1lfZRUlKiwsJCrxsAADh/VWtAKSsr05gxY9SxY0e1atVKkpSXl6fatWsrIiLCa97IyEjl5eVV2k9WVpbCw8OdW2xsbHWWDQAA/KxaA8rIkSP12Wef6eWXX/5V/WRkZKigoMC57dy500cVAgAAG/n8GJRyd999t9555x2tXr1aF110kdMeFRWlI0eOaP/+/V6jKPn5+YqKiqq0L4/HI4/HU12lAgAAy/h8BMUYo7vvvltvvPGGVq5cqbi4OK/pbdq0Ua1atZSTk+O0bdu2Td9//72SkpJ8XQ4AAKiBfD6CMnLkSC1cuFBvvvmmQkNDneNKwsPDFRwcrPDwcA0bNkzp6emqX7++wsLCdM899ygpKYkzeAAAgKRqCChPP/20JOmPf/yjV/vcuXM1ePBgSdKMGTPkdrvVp08flZSUKCUlRbNnz/Z1KQAAoIbyeUAxxpxxnqCgIM2aNUuzZs3y9cMDAIDzAL/FAwAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACs49eAMmvWLF1yySUKCgpSYmKiPv74Y3+WAwAALBHorwd+5ZVXlJ6erjlz5igxMVHZ2dlKSUnRtm3b1KhRI3+V5ReusmMy/i7i98wYqeyYv6sA7OUOlFwuf1fxu+X6nf5/chlj/PLemJiYqHbt2umpp56SJJWVlSk2Nlb33HOPxo8f7zVvSUmJSkpKnPuFhYWKjY1VQUGBwsLCftO6feXQoUNKTU31dxkAgBpk6dKlCg4O9ncZ56ywsFDh4eFVev/2y1c8R44c0caNG5WcnPx/hbjdSk5O1tq1ayvMn5WVpfDwcOcWGxv7W5YLAAB+Y375imfPnj0qLS1VZGSkV3tkZKS+/PLLCvNnZGQoPT3duV8+glKTBQUFaenSpf4uA5KMMV4jdAC8eTweufiKxwpBQUH+LuE347djUM6Gx+ORx+Pxdxk+5XK5avQw3fkmJCTE3yUAAE7gl694GjZsqICAAOXn53u15+fnKyoqyh8lAQAAi/gloNSuXVtt2rRRTk6O01ZWVqacnBwlJSX5oyQAAGARv33Fk56errS0NLVt21bt27dXdna2iouLNWTIEH+VBAAALOG3gNK/f3/9/PPPyszMVF5enhISErRs2bIKB84CAIDfH79dB+XXOJvzqAEAgB2svw4KAADA6RBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWqRG/Znyy8mvLFRYW+rkSAABQVeXv21W5RmyNDChFRUWSpNjYWD9XAgAAzlZRUZHCw8NPO0+NvNR9WVmZdu3apdDQULlcLn+XA8CHCgsLFRsbq507d/JTFsB5xhijoqIixcTEyO0+/VEmNTKgADh/8VtbACQOkgUAABYioAAAAOsQUABYxePxaOLEifJ4PP4uBYAfcQwKAACwDiMoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABYJVZs2bpkksuUVBQkBITE/Xxxx/7uyQAfkBAAWCNV155Renp6Zo4caI2bdqkK664QikpKdq9e7e/SwPwG+M6KACskZiYqHbt2umpp56SdPyHQWNjY3XPPfdo/Pjxfq4OwG+JERQAVjhy5Ig2btyo5ORkp83tdis5OVlr1671Y2UA/IGAAsAKe/bsUWlpqSIjI73aIyMjlZeX56eqAPgLAQUAAFiHgALACg0bNlRAQIDy8/O92vPz8xUVFeWnqgD4CwEFgBVq166tNm3aKCcnx2krKytTTk6OkpKS/FgZAH8I9HcBAFAuPT1daWlpatu2rdq3b6/s7GwVFxdryJAh/i4NwG+MgALAGv3799fPP/+szMxM5eXlKSEhQcuWLatw4CyA8x/XQQEAANbhGBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWOf/AfeyZT66q5/+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = evaluate_exact(\n",
    "    data_qa_train[\"answer_text\"].to_list(),\n",
    "    list(map(lambda p: p[\"answer\"], predictions)),\n",
    ")\n",
    "print(np.mean(res))\n",
    "sns.boxplot(res)\n",
    "plt.title(\"Ground truth vs prediction exact match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f5138651",
   "metadata": {},
   "outputs": [],
   "source": [
    "_preprocess_fn = lambda x: x.strip(\" .,!?\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c6713e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_jury(predictions: List[str], references: List[str]) -> dict:\n",
    "    eval_1 = Jury()(predictions=predictions, references=references)\n",
    "    eval_2 = Jury(metrics=[\"exact_match\", \"bleu\", \"squad\"])(predictions=predictions, references=references)\n",
    "    eval_2.update(eval_1)\n",
    "    return eval_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b92a3cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_items': 102,\n",
       " 'empty_items': 0,\n",
       " 'exact_match': 0.47058823529411764,\n",
       " 'bleu': {'score': 0.4377416264384578,\n",
       "  'precisions': [0.9660297239915074,\n",
       "   0.962059620596206,\n",
       "   0.9581881533101045,\n",
       "   0.9547511312217195],\n",
       "  'brevity_penalty': 0.4558631461080209,\n",
       "  'length_ratio': 0.5600475624256838,\n",
       "  'translation_length': 471,\n",
       "  'reference_length': 841},\n",
       " 'squad': {'exact_match': 0.7156862745098038, 'f1': 0.8327083419560766},\n",
       " 'bleu_1': {'score': 0.44037734921263166,\n",
       "  'precisions': [0.9660297239915074],\n",
       "  'brevity_penalty': 0.4558631461080209,\n",
       "  'length_ratio': 0.5600475624256838,\n",
       "  'translation_length': 471,\n",
       "  'reference_length': 841},\n",
       " 'bleu_2': {'score': 0.43947150565346144,\n",
       "  'precisions': [0.9660297239915074, 0.962059620596206],\n",
       "  'brevity_penalty': 0.4558631461080209,\n",
       "  'length_ratio': 0.5600475624256838,\n",
       "  'translation_length': 471,\n",
       "  'reference_length': 841},\n",
       " 'bleu_3': {'score': 0.43858008555336175,\n",
       "  'precisions': [0.9660297239915074, 0.962059620596206, 0.9581881533101045],\n",
       "  'brevity_penalty': 0.4558631461080209,\n",
       "  'length_ratio': 0.5600475624256838,\n",
       "  'translation_length': 471,\n",
       "  'reference_length': 841},\n",
       " 'bleu_4': {'score': 0.4377416264384578,\n",
       "  'precisions': [0.9660297239915074,\n",
       "   0.962059620596206,\n",
       "   0.9581881533101045,\n",
       "   0.9547511312217195],\n",
       "  'brevity_penalty': 0.4558631461080209,\n",
       "  'length_ratio': 0.5600475624256838,\n",
       "  'translation_length': 471,\n",
       "  'reference_length': 841},\n",
       " 'meteor': {'score': 0.7517334590594303},\n",
       " 'rouge': {'rouge1': 0.8323209793724882,\n",
       "  'rouge2': 0.7063001800092542,\n",
       "  'rougeL': 0.8313858582864919,\n",
       "  'rougeLsum': 0.8329738533296864}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "evaluate_jury(\n",
    "    predictions=list(map(lambda p: p[\"answer\"], predictions)),\n",
    "    references=data_qa_train[\"answer_text\"].to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f9e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
